Implementações:
	+ Viterbi decoder (só usar o do CRF)
	+ CRF (só empilhar nos já existentes)
	+ Attention (só empilhar nos já existentes)
	+ Transformer (só empilhar nos já existentes)
	+ Dropout depois de quais camadas? (ablation)
	+ LayerNormalization depois de quais camadas? (ablation)

	+ dividir modelos em: representation, body, head
	+ transfer learning


I need a GPU and more RAM!!!


Avaliações:	
	l1 = sparsity -> adicionar nn.l1loss aos parametros
	l2 = weight decay -> focar nisso 


	Testar com diferentes word embeddings (ablation) 
		- polyglot
		- word2vec
		- fasttext
		- glove
		- fonseca
	

Descartadas:
	Adicionar BPE vocab (não tem bons resultados pra PoS tagging):
	https://pdfs.semanticscholar.org/87b8/60da2501169aafafdfa0cac18e6779a198c1.pdf


Nova organização:
----
Model:
	__init__: fields, options
	init_weights
	forward: (bs, seq_len) -> (bs, seq_len, nb_classes)
	save
	load
	freeze
	predict
	loss

	Representation:
		__init__: fields, options
			vocab = fields['word'].vocab
			vai ter um pequeno overhead pq toda época precisamos remapear a entrada
			para o espaço das representações, massssss, o torchtext já numericaliza os
			batches todas as vezes de qualquer maneira, então não vai mudar muita coisa:
			o treino vai demorar uns 5 segundos a mais por época, o que acaba dando uns +/- 5mins a mais
			dependendo do numero de épocas e do tamanhos das sentenças

		_init_weights
		get_offsets: 
			- None for normal tokenization
			- [ids] for subword units
		forward: (bs, seq_len) -> (bs, rep_seq_len, rep_size) if keep_subword_units is True
							   -> (bs, seq_len, rep_size) otherwise

			 if keep_subword_units is False (default):
			      output = output[self.get_offsets(input)]
		load
		save
		freeze

	Body:
		__init__: options
		_init_weights
		forward: (bs, seq_len*, rep_size) -> (bs, seq_len*, hidden_size)
		load
		save
		freeze

	Head:
		__init__: options
		forward: (bs, rep_seq_len, hidden_size) if keep_subword_units is True
		         (bs, seq_len*, hidden_size) otherwise
		         -> (bs, seq_len, nb_classes)

		         if keep_word_units is True:
		         	output = output[representation.get_offsets(input)]
		_init_weights
		load
		save
		freeze
		predict_classes
		predict_probas
		loss
