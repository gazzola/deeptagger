Implementações:
	+ Viterbi decoder (só usar o do CRF)
	+ CRF (só empilhar nos já existentes)
	+ Attention (só empilhar nos já existentes)
	+ Transformer (só empilhar nos já existentes)
	+ Dropout depois de quais camadas? (ablation)
	+ LayerNormalization depois de quais camadas? (ablation)

	+ dividir modelos em: representation, body, head
	+ transfer learning


I need a GPU and more RAM!!!


Avaliações:	
	l1 = sparsity -> adicionar nn.l1loss aos parametros
	l2 = weight decay -> focar nisso 


	Testar com diferentes word embeddings (ablation) 
		- polyglot
		- word2vec
		- fasttext
		- glove
		- fonseca
	

Descartadas:
	Adicionar BPE vocab (não tem bons resultados pra PoS tagging):
	https://pdfs.semanticscholar.org/87b8/60da2501169aafafdfa0cac18e6779a198c1.pdf


Nova organização:
----
Model:
	__init__: fields, options
	init_weights
	forward: (bs, seq_len) -> (bs, seq_len, nb_classes)
	save
	load
	freeze
	predict
	loss

	Representation:
		__init__: fields, options
			vocab = fields['word'].vocab
			vai ter um pequeno overhead pq toda época precisamos remapear a entrada
			para o espaço das representações, massssss, o torchtext já numericaliza os
			batches todas as vezes de qualquer maneira, então não vai mudar muita coisa

		_init_weights
		map: word ids to a
		get_offsets: 
			- None for normal tokenization
			- [ids] for subword units
		forward: (bs, seq_len) -> (bs, rep_seq_len, rep_size) -> (bs, seq_len, rep_size)
		load
		save
		freeze

	Body:
		__init__: options
		_init_weights
		forward: (bs, seq_len, rep_size) -> (bs, seq_len, hidden_size)
		load
		save
		freeze

	Head:
		__init__: options
		forward: (bs, seq_len, hidden_size) -> (bs, seq_len, nb_classes)
		_init_weights
		load
		save
		freeze
		predict_classes
		predict_probas
		loss
